{
    "mmlu_step_by_step": {
        "perplexity": 4.593380472660065,
        "IDF_score": 0.729,
        "log_propability": -451.0,
        "skywork_reward_score": 12.952734375,
        "CAR_score": 2.36
    },
    "mmlu_claude": {
        "perplexity": 3.266204364299774,
        "IDF_score": 0.653,
        "log_propability": -276.0,
        "skywork_reward_score": 15.452470703125,
        "CAR_score": 3.43
    },
    "mmlu_gpt4_style_in_context_examples": {
        "perplexity": 4.489022047519684,
        "IDF_score": 0.735,
        "log_propability": -554.0,
        "skywork_reward_score": 20.4858203125,
        "CAR_score": 3.8
    },
    "mmlu_gpt4": {
        "perplexity": 4.496119010448456,
        "IDF_score": 0.691,
        "log_propability": -344.0,
        "skywork_reward_score": 10.4716796875,
        "CAR_score": 1.94
    },
    "mmlu_mini_gpt4": {
        "perplexity": 4.1033154904842375,
        "IDF_score": 0.66,
        "log_propability": -282.0,
        "skywork_reward_score": 8.6827880859375,
        "CAR_score": 1.68
    },
    "mmlu_groundtruth": {
        "perplexity": 27.117273769378663,
        "IDF_score": 0.597,
        "log_propability": -12.9,
        "skywork_reward_score": -4.884662475585937,
        "CAR_score": -0.456
    },
    "mmlu_openai_human_written_examples": {
        "perplexity": 4.790992815494537,
        "IDF_score": 0.724,
        "log_propability": -442.0,
        "skywork_reward_score": 14.888515625,
        "CAR_score": 2.66
    }
}