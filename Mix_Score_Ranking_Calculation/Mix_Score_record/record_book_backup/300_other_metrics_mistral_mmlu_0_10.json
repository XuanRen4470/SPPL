{
    "mmlu_step_by_step": {
        "perplexity": 4.30109965801239,
        "IDF_score": 0.735,
        "log_propability": -451.0,
        "skywork_reward_score": 10.6390625,
        "CAR_score": 2.02
    },
    "mmlu_claude": {
        "perplexity": 3.0440310716629027,
        "IDF_score": 0.646,
        "log_propability": -298.0,
        "skywork_reward_score": 15.4625,
        "CAR_score": 3.59
    },
    "mmlu_gpt4_style_in_context_examples": {
        "perplexity": 3.880476140975952,
        "IDF_score": 0.745,
        "log_propability": -591.0,
        "skywork_reward_score": 19.525,
        "CAR_score": 3.89
    },
    "mmlu_gpt4": {
        "perplexity": 4.524125766754151,
        "IDF_score": 0.737,
        "log_propability": -457.0,
        "skywork_reward_score": 8.8296875,
        "CAR_score": 1.64
    },
    "mmlu_mini_gpt4": {
        "perplexity": 4.083730888366699,
        "IDF_score": 0.675,
        "log_propability": -345.0,
        "skywork_reward_score": 5.60406494140625,
        "CAR_score": 1.09
    },
    "mmlu_groundtruth": {
        "perplexity": 22691.903430175782,
        "IDF_score": 1.46,
        "log_propability": -36.8,
        "skywork_reward_score": -3.01776123046875,
        "CAR_score": -0.105
    },
    "mmlu_openai_human_written_examples": {
        "perplexity": 4.911047315597534,
        "IDF_score": 0.817,
        "log_propability": -538.0,
        "skywork_reward_score": 11.0984375,
        "CAR_score": 1.94
    }
}