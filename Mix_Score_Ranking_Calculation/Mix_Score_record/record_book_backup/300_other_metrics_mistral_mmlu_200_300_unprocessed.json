{
    "mmlu_step_by_step": {
        "perplexity": 4.573979027271271,
        "IDF_score": 0.787,
        "log_propability": -548.0,
        "skywork_reward_score": 12.26854248046875,
        "CAR_score": 2.23
    },
    "mmlu_claude": {
        "perplexity": 3.1991245937347412,
        "IDF_score": 0.674,
        "log_propability": -324.0,
        "skywork_reward_score": 14.29076171875,
        "CAR_score": 3.21
    },
    "mmlu_gpt4_style_in_context_examples": {
        "perplexity": 4.290794847011566,
        "IDF_score": 0.775,
        "log_propability": -602.0,
        "skywork_reward_score": 18.518203125,
        "CAR_score": 3.51
    },
    "mmlu_gpt4": {
        "perplexity": 4.975044348239899,
        "IDF_score": 0.767,
        "log_propability": -447.0,
        "skywork_reward_score": 10.91873046875,
        "CAR_score": 1.91
    },
    "mmlu_mini_gpt4": {
        "perplexity": 4.295166788101196,
        "IDF_score": 0.692,
        "log_propability": -325.0,
        "skywork_reward_score": 9.2425,
        "CAR_score": 1.74
    },
    "mmlu_groundtruth": {
        "perplexity": 42946.96481262207,
        "IDF_score": 1.47,
        "log_propability": -37.0,
        "skywork_reward_score": -5.1030712890625,
        "CAR_score": -0.178
    },
    "mmlu_openai_human_written_examples": {
        "perplexity": 4.842121291160583,
        "IDF_score": 0.772,
        "log_propability": -534.0,
        "skywork_reward_score": 15.219393920898437,
        "CAR_score": 2.7
    }
}