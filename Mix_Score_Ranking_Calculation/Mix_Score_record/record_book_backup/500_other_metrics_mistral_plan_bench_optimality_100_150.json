{
    "plan_bench_optimality_step_by_step": {
        "perplexity": 2.1717070627212522,
        "IDF_score": 0.733,
        "log_propability": -361.0,
        "skywork_reward_score": -7.439677734375,
        "CAR_score": -2.28,
        "cos_similarity": 0.843251953125
    },
    "plan_bench_optimality_claude": {
        "perplexity": 2.3790219688415526,
        "IDF_score": 0.627,
        "log_propability": -235.0,
        "skywork_reward_score": -7.439677734375,
        "CAR_score": -2.09,
        "cos_similarity": 0.84177734375
    },
    "plan_bench_optimality_gpt4_style_in_context_examples": {
        "perplexity": 2.020154211521149,
        "IDF_score": 0.763,
        "log_propability": -339.0,
        "skywork_reward_score": -7.439677734375,
        "CAR_score": -2.43,
        "cos_similarity": 0.820322265625
    },
    "plan_bench_optimality_gpt4": {
        "perplexity": 2.3811725616455077,
        "IDF_score": 0.718,
        "log_propability": -377.0,
        "skywork_reward_score": -7.439677734375,
        "CAR_score": -2.1,
        "cos_similarity": 0.86228515625
    },
    "plan_bench_optimality_mini_gpt4": {
        "perplexity": 2.516996567249298,
        "IDF_score": 0.693,
        "log_propability": -383.0,
        "skywork_reward_score": -7.439677734375,
        "CAR_score": -2.01,
        "cos_similarity": 0.87390625
    },
    "plan_bench_optimality_groundtruth": {
        "perplexity": 136.75805052280427,
        "IDF_score": 1.3,
        "log_propability": -130.0,
        "skywork_reward_score": -7.439677734375,
        "CAR_score": -0.794,
        "cos_similarity": 0.5033740234375
    },
    "plan_bench_optimality_openai_human_written_examples": {
        "perplexity": 3.1244888138771056,
        "IDF_score": 0.674,
        "log_propability": -357.0,
        "skywork_reward_score": -7.439677734375,
        "CAR_score": -1.72,
        "cos_similarity": 0.86966796875
    },
    "plan_bench_optimality_rewrite_groundtruth_in_own_words": {
        "perplexity": 3.144002914428711,
        "IDF_score": 0.655,
        "log_propability": -321.0,
        "skywork_reward_score": -7.439677734375,
        "CAR_score": -1.73,
        "cos_similarity": 0.86517578125
    }
}