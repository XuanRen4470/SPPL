{
    "mmlu_pro_law_step_by_step": {
        "perplexity": 4.69975254535675,
        "IDF_score": 0.799,
        "log_propability": -687.0,
        "skywork_reward_score": 9.73359375,
        "CAR_score": 1.74
    },
    "mmlu_pro_law_claude": {
        "perplexity": 3.43494393825531,
        "IDF_score": 0.747,
        "log_propability": -316.0,
        "skywork_reward_score": 8.191015625,
        "CAR_score": 1.75
    },
    "mmlu_pro_law_gpt4_style_in_context_examples": {
        "perplexity": 4.867117691040039,
        "IDF_score": 0.774,
        "log_propability": -422.0,
        "skywork_reward_score": 10.41484375,
        "CAR_score": 1.83
    },
    "mmlu_pro_law_gpt4": {
        "perplexity": 5.769422197341919,
        "IDF_score": 0.827,
        "log_propability": -626.0,
        "skywork_reward_score": 4.47109375,
        "CAR_score": 0.725
    },
    "mmlu_pro_law_mini_gpt4": {
        "perplexity": 4.672082018852234,
        "IDF_score": 0.708,
        "log_propability": -344.0,
        "skywork_reward_score": 2.8080078125,
        "CAR_score": 0.507
    },
    "mmlu_pro_law_groundtruth": {
        "perplexity": 17334.16866455078,
        "IDF_score": 1.47,
        "log_propability": -37.5,
        "skywork_reward_score": -17.125,
        "CAR_score": -0.588
    },
    "mmlu_pro_law_openai_human_written_examples": {
        "perplexity": 6.516077375411987,
        "IDF_score": 0.753,
        "log_propability": -283.0,
        "skywork_reward_score": 4.821240234375,
        "CAR_score": 0.748
    }
}