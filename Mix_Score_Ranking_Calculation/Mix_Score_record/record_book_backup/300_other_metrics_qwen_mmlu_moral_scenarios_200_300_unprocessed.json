{
    "mmlu_moral_scenarios_step_by_step": {
        "perplexity": 3.0650520944595336,
        "IDF_score": 0.626,
        "log_propability": -264.0,
        "skywork_reward_score": 10.78623046875,
        "CAR_score": 2.5
    },
    "mmlu_moral_scenarios_claude": {
        "perplexity": 2.4727636992931368,
        "IDF_score": 0.484,
        "log_propability": -146.0,
        "skywork_reward_score": 9.087021484375,
        "CAR_score": 2.47
    },
    "mmlu_moral_scenarios_gpt4_style_in_context_examples": {
        "perplexity": 3.1002977752685545,
        "IDF_score": 0.595,
        "log_propability": -247.0,
        "skywork_reward_score": 10.798583984375,
        "CAR_score": 2.47
    },
    "mmlu_moral_scenarios_gpt4": {
        "perplexity": 3.3709285378456117,
        "IDF_score": 0.567,
        "log_propability": -193.0,
        "skywork_reward_score": 5.6010107421875,
        "CAR_score": 1.22
    },
    "mmlu_moral_scenarios_mini_gpt4": {
        "perplexity": 3.100708615779877,
        "IDF_score": 0.517,
        "log_propability": -162.0,
        "skywork_reward_score": 4.192479248046875,
        "CAR_score": 0.969
    },
    "mmlu_moral_scenarios_groundtruth": {
        "perplexity": 1454.7672354125978,
        "IDF_score": 2.0,
        "log_propability": -27.6,
        "skywork_reward_score": -7.34066162109375,
        "CAR_score": -0.338
    },
    "mmlu_moral_scenarios_openai_human_written_examples": {
        "perplexity": 3.890483696460724,
        "IDF_score": 0.488,
        "log_propability": -120.0,
        "skywork_reward_score": 3.84029296875,
        "CAR_score": 0.77
    }
}