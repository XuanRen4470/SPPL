{
    "mmlu_moral_scenarios_step_by_step": {
        "perplexity": 3.2120634078979493,
        "IDF_score": 0.64,
        "log_propability": -284.0,
        "skywork_reward_score": 11.187890625,
        "CAR_score": 2.53
    },
    "mmlu_moral_scenarios_claude": {
        "perplexity": 2.312962090969086,
        "IDF_score": 0.44,
        "log_propability": -127.0,
        "skywork_reward_score": 11.86875,
        "CAR_score": 3.42
    },
    "mmlu_moral_scenarios_gpt4_style_in_context_examples": {
        "perplexity": 3.1892289400100706,
        "IDF_score": 0.608,
        "log_propability": -267.0,
        "skywork_reward_score": 10.2015625,
        "CAR_score": 2.3
    },
    "mmlu_moral_scenarios_gpt4": {
        "perplexity": 3.3983570337295532,
        "IDF_score": 0.578,
        "log_propability": -192.0,
        "skywork_reward_score": 5.8791015625,
        "CAR_score": 1.27
    },
    "mmlu_moral_scenarios_mini_gpt4": {
        "perplexity": 3.2233402967453,
        "IDF_score": 0.546,
        "log_propability": -171.0,
        "skywork_reward_score": 7.88671875,
        "CAR_score": 1.77
    },
    "mmlu_moral_scenarios_groundtruth": {
        "perplexity": 1162.6528747558593,
        "IDF_score": 1.96,
        "log_propability": -27.3,
        "skywork_reward_score": -6.7255859375,
        "CAR_score": -0.314
    },
    "mmlu_moral_scenarios_openai_human_written_examples": {
        "perplexity": 4.176194572448731,
        "IDF_score": 0.527,
        "log_propability": -138.0,
        "skywork_reward_score": 4.4232421875,
        "CAR_score": 0.853
    }
}