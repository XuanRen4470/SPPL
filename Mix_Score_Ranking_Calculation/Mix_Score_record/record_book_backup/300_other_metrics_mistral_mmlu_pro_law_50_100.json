{
    "mmlu_pro_law_step_by_step": {
        "perplexity": 4.4348687744140625,
        "IDF_score": 0.796,
        "log_propability": -604.0,
        "skywork_reward_score": 12.2369140625,
        "CAR_score": 2.28
    },
    "mmlu_pro_law_claude": {
        "perplexity": 3.274990062713623,
        "IDF_score": 0.699,
        "log_propability": -337.0,
        "skywork_reward_score": 11.92453125,
        "CAR_score": 2.65
    },
    "mmlu_pro_law_gpt4_style_in_context_examples": {
        "perplexity": 5.030438055992127,
        "IDF_score": 0.779,
        "log_propability": -487.0,
        "skywork_reward_score": 12.406328125,
        "CAR_score": 2.19
    },
    "mmlu_pro_law_gpt4": {
        "perplexity": 5.461341123580933,
        "IDF_score": 0.791,
        "log_propability": -496.0,
        "skywork_reward_score": 8.0476708984375,
        "CAR_score": 1.35
    },
    "mmlu_pro_law_mini_gpt4": {
        "perplexity": 4.47683497428894,
        "IDF_score": 0.721,
        "log_propability": -346.0,
        "skywork_reward_score": 6.034375,
        "CAR_score": 1.12
    },
    "mmlu_pro_law_groundtruth": {
        "perplexity": 18030.01772338867,
        "IDF_score": 1.38,
        "log_propability": -35.6,
        "skywork_reward_score": -12.7171875,
        "CAR_score": -0.46
    },
    "mmlu_pro_law_openai_human_written_examples": {
        "perplexity": 6.1716522073745725,
        "IDF_score": 0.759,
        "log_propability": -294.0,
        "skywork_reward_score": 6.445859375,
        "CAR_score": 1.03
    }
}