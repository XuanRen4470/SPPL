{
    "winogrande_step_by_step": {
        "perplexity": 4.942397545576096,
        "IDF_score": 0.609,
        "log_propability": -365.0,
        "skywork_reward_score": 11.346097513834636,
        "CAR_score": 1.99,
        "cos_similarity": 0.703177490234375
    },
    "winogrande_claude": {
        "perplexity": 4.1097708463668825,
        "IDF_score": 0.503,
        "log_propability": -222.0,
        "skywork_reward_score": 11.346097513834636,
        "CAR_score": 2.2,
        "cos_similarity": 0.7208135986328125
    },
    "winogrande_gpt4_style_in_context_examples": {
        "perplexity": 4.933339363336563,
        "IDF_score": 0.417,
        "log_propability": -212.0,
        "skywork_reward_score": 11.346097513834636,
        "CAR_score": 2.0,
        "cos_similarity": 0.735400390625
    },
    "winogrande_gpt4": {
        "perplexity": 6.385027426481247,
        "IDF_score": 0.404,
        "log_propability": -188.0,
        "skywork_reward_score": 11.346097513834636,
        "CAR_score": 1.8,
        "cos_similarity": 0.7454443359375
    },
    "winogrande_mini_gpt4": {
        "perplexity": 5.843738659620285,
        "IDF_score": 0.381,
        "log_propability": -191.0,
        "skywork_reward_score": 11.346097513834636,
        "CAR_score": 1.85,
        "cos_similarity": 0.763052978515625
    },
    "winogrande_groundtruth": {
        "perplexity": 69565.11350952148,
        "IDF_score": 552.0,
        "log_propability": -10.3,
        "skywork_reward_score": 11.346097513834636,
        "CAR_score": 0.346,
        "cos_similarity": 0.4064544677734375
    },
    "winogrande_openai_human_written_examples": {
        "perplexity": 6.234314414262772,
        "IDF_score": 0.247,
        "log_propability": -123.0,
        "skywork_reward_score": 11.346097513834636,
        "CAR_score": 1.84,
        "cos_similarity": 0.7552685546875
    }
}