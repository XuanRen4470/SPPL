{
    "mmlu_moral_scenarios_step_by_step": {
        "perplexity": 3.4430253585179647,
        "IDF_score": 0.627,
        "log_propability": -310.0,
        "skywork_reward_score": 9.590625,
        "CAR_score": 2.05
    },
    "mmlu_moral_scenarios_claude": {
        "perplexity": 3.1202534357706706,
        "IDF_score": 0.572,
        "log_propability": -218.0,
        "skywork_reward_score": 9.571875,
        "CAR_score": 2.19
    },
    "mmlu_moral_scenarios_gpt4_style_in_context_examples": {
        "perplexity": 3.174263048171997,
        "IDF_score": 0.605,
        "log_propability": -306.0,
        "skywork_reward_score": 9.760677083333333,
        "CAR_score": 2.2
    },
    "mmlu_moral_scenarios_gpt4": {
        "perplexity": 4.017985026041667,
        "IDF_score": 0.595,
        "log_propability": -247.0,
        "skywork_reward_score": 6.076497395833333,
        "CAR_score": 1.19
    },
    "mmlu_moral_scenarios_mini_gpt4": {
        "perplexity": 3.4845142126083375,
        "IDF_score": 0.554,
        "log_propability": -200.0,
        "skywork_reward_score": 6.267464192708333,
        "CAR_score": 1.34
    },
    "mmlu_moral_scenarios_groundtruth": {
        "perplexity": 40286.41278483073,
        "IDF_score": 1.55,
        "log_propability": -38.8,
        "skywork_reward_score": -6.165885416666667,
        "CAR_score": -0.205
    },
    "mmlu_moral_scenarios_openai_human_written_examples": {
        "perplexity": 4.492271407445272,
        "IDF_score": 0.551,
        "log_propability": -164.0,
        "skywork_reward_score": 2.8799153645833333,
        "CAR_score": 0.538
    }
}