{
    "mmlu_step_by_step": {
        "perplexity": 4.459462099075317,
        "IDF_score": 0.662,
        "log_propability": -522.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.71,
        "cos_similarity": 0.851767578125
    },
    "mmlu_claude": {
        "perplexity": 3.0430319929122924,
        "IDF_score": 0.575,
        "log_propability": -311.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 3.42,
        "cos_similarity": 0.873515625
    },
    "mmlu_gpt4_style_in_context_examples": {
        "perplexity": 4.274717831611634,
        "IDF_score": 0.666,
        "log_propability": -609.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.81,
        "cos_similarity": 0.84828125
    },
    "mmlu_gpt4": {
        "perplexity": 5.109856066703796,
        "IDF_score": 0.623,
        "log_propability": -448.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.54,
        "cos_similarity": 0.857216796875
    },
    "mmlu_mini_gpt4": {
        "perplexity": 4.318769769668579,
        "IDF_score": 0.523,
        "log_propability": -328.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.76,
        "cos_similarity": 0.87205078125
    },
    "mmlu_groundtruth": {
        "perplexity": 45373.784936523436,
        "IDF_score": 85.4,
        "log_propability": -36.7,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 0.515,
        "cos_similarity": 0.13958740234375
    },
    "mmlu_openai_human_written_examples": {
        "perplexity": 4.955962152481079,
        "IDF_score": 0.633,
        "log_propability": -503.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.58,
        "cos_similarity": 0.856103515625
    }
}