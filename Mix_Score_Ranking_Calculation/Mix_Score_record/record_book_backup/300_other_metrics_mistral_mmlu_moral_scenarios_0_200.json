{
    "mmlu_moral_scenarios_step_by_step": {
        "perplexity": 3.5075539076328277,
        "IDF_score": 0.634,
        "log_propability": -326.0,
        "skywork_reward_score": 8.49802978515625,
        "CAR_score": 1.81
    },
    "mmlu_moral_scenarios_claude": {
        "perplexity": 2.981273807287216,
        "IDF_score": 0.536,
        "log_propability": -202.0,
        "skywork_reward_score": 8.51337158203125,
        "CAR_score": 2.01
    },
    "mmlu_moral_scenarios_gpt4_style_in_context_examples": {
        "perplexity": 3.3484629249572753,
        "IDF_score": 0.621,
        "log_propability": -318.0,
        "skywork_reward_score": 8.063497924804688,
        "CAR_score": 1.76
    },
    "mmlu_moral_scenarios_gpt4": {
        "perplexity": 4.052449103593826,
        "IDF_score": 0.604,
        "log_propability": -255.0,
        "skywork_reward_score": 4.068388671875,
        "CAR_score": 0.794
    },
    "mmlu_moral_scenarios_mini_gpt4": {
        "perplexity": 3.7033222913742065,
        "IDF_score": 0.556,
        "log_propability": -212.0,
        "skywork_reward_score": 5.287353515625,
        "CAR_score": 1.09
    },
    "mmlu_moral_scenarios_groundtruth": {
        "perplexity": 55388.8523614502,
        "IDF_score": 1.58,
        "log_propability": -39.8,
        "skywork_reward_score": -7.609716796875,
        "CAR_score": -0.247
    },
    "mmlu_moral_scenarios_openai_human_written_examples": {
        "perplexity": 4.767742816209793,
        "IDF_score": 0.562,
        "log_propability": -173.0,
        "skywork_reward_score": 2.19342529296875,
        "CAR_score": 0.396
    }
}