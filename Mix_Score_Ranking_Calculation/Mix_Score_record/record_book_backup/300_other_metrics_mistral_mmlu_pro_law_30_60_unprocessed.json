{
    "mmlu_pro_law_step_by_step": {
        "perplexity": 4.337634030977885,
        "IDF_score": 0.796,
        "log_propability": -534.0,
        "skywork_reward_score": 7.818033854166667,
        "CAR_score": 1.48
    },
    "mmlu_pro_law_claude": {
        "perplexity": 3.303561067581177,
        "IDF_score": 0.708,
        "log_propability": -321.0,
        "skywork_reward_score": 4.568619791666666,
        "CAR_score": 1.0
    },
    "mmlu_pro_law_gpt4_style_in_context_examples": {
        "perplexity": 5.207374167442322,
        "IDF_score": 0.771,
        "log_propability": -490.0,
        "skywork_reward_score": 8.933463541666667,
        "CAR_score": 1.55
    },
    "mmlu_pro_law_gpt4": {
        "perplexity": 5.317561356226603,
        "IDF_score": 0.796,
        "log_propability": -424.0,
        "skywork_reward_score": 5.8822265625,
        "CAR_score": 0.996
    },
    "mmlu_pro_law_mini_gpt4": {
        "perplexity": 4.104076480865478,
        "IDF_score": 0.703,
        "log_propability": -349.0,
        "skywork_reward_score": 4.849739583333333,
        "CAR_score": 0.942
    },
    "mmlu_pro_law_groundtruth": {
        "perplexity": 26567.329286702476,
        "IDF_score": 1.44,
        "log_propability": -37.3,
        "skywork_reward_score": -14.169791666666667,
        "CAR_score": -0.49
    },
    "mmlu_pro_law_openai_human_written_examples": {
        "perplexity": 6.263659707705179,
        "IDF_score": 0.764,
        "log_propability": -287.0,
        "skywork_reward_score": 4.566438802083334,
        "CAR_score": 0.718
    }
}