{
    "mmlu_step_by_step": {
        "perplexity": 4.440055342912674,
        "IDF_score": 0.715,
        "log_propability": -441.0,
        "skywork_reward_score": 12.86133056640625,
        "CAR_score": 2.39
    },
    "mmlu_claude": {
        "perplexity": 3.21581216275692,
        "IDF_score": 0.648,
        "log_propability": -277.0,
        "skywork_reward_score": 15.4934228515625,
        "CAR_score": 3.47
    },
    "mmlu_gpt4_style_in_context_examples": {
        "perplexity": 4.512448507547378,
        "IDF_score": 0.73,
        "log_propability": -552.0,
        "skywork_reward_score": 19.58830078125,
        "CAR_score": 3.62
    },
    "mmlu_gpt4": {
        "perplexity": 4.689737430810928,
        "IDF_score": 0.694,
        "log_propability": -361.0,
        "skywork_reward_score": 10.2419921875,
        "CAR_score": 1.86
    },
    "mmlu_mini_gpt4": {
        "perplexity": 4.2061497849226,
        "IDF_score": 0.664,
        "log_propability": -292.0,
        "skywork_reward_score": 8.234583129882813,
        "CAR_score": 1.57
    },
    "mmlu_groundtruth": {
        "perplexity": 27.66978708267212,
        "IDF_score": 0.601,
        "log_propability": -13.0,
        "skywork_reward_score": -5.211920471191406,
        "CAR_score": -0.484
    },
    "mmlu_openai_human_written_examples": {
        "perplexity": 4.911661661863327,
        "IDF_score": 0.719,
        "log_propability": -445.0,
        "skywork_reward_score": 14.4209130859375,
        "CAR_score": 2.55
    }
}