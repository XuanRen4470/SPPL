{
    "mmlu_step_by_step": {
        "perplexity": 3.8614566558599472,
        "IDF_score": 0.724,
        "log_propability": -402.0,
        "skywork_reward_score": 12.86133056640625,
        "CAR_score": 2.59
    },
    "mmlu_claude": {
        "perplexity": 2.7815747755765914,
        "IDF_score": 0.623,
        "log_propability": -243.0,
        "skywork_reward_score": 15.4934228515625,
        "CAR_score": 3.85
    },
    "mmlu_gpt4_style_in_context_examples": {
        "perplexity": 3.6578545105457305,
        "IDF_score": 0.726,
        "log_propability": -473.0,
        "skywork_reward_score": 19.58830078125,
        "CAR_score": 4.1
    },
    "mmlu_gpt4": {
        "perplexity": 4.016370493769646,
        "IDF_score": 0.692,
        "log_propability": -323.0,
        "skywork_reward_score": 10.2419921875,
        "CAR_score": 2.02
    },
    "mmlu_mini_gpt4": {
        "perplexity": 3.3517217046022414,
        "IDF_score": 0.607,
        "log_propability": -247.0,
        "skywork_reward_score": 8.234583129882813,
        "CAR_score": 1.8
    },
    "mmlu_groundtruth": {
        "perplexity": 3250.5662873077395,
        "IDF_score": 2.12,
        "log_propability": -29.3,
        "skywork_reward_score": -5.211920471191406,
        "CAR_score": -0.227
    },
    "mmlu_openai_human_written_examples": {
        "perplexity": 4.131422135829926,
        "IDF_score": 0.713,
        "log_propability": -396.0,
        "skywork_reward_score": 14.4209130859375,
        "CAR_score": 2.81
    }
}