{
    "mmlu_pro_law_step_by_step": {
        "perplexity": 4.586513934135437,
        "IDF_score": 0.71,
        "log_propability": -621.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.928,
        "cos_similarity": 0.81228515625
    },
    "mmlu_pro_law_claude": {
        "perplexity": 3.274990062713623,
        "IDF_score": 0.61,
        "log_propability": -337.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 1.12,
        "cos_similarity": 0.8419140625
    },
    "mmlu_pro_law_gpt4_style_in_context_examples": {
        "perplexity": 5.030438055992127,
        "IDF_score": 0.658,
        "log_propability": -487.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.893,
        "cos_similarity": 0.831787109375
    },
    "mmlu_pro_law_gpt4": {
        "perplexity": 5.461341123580933,
        "IDF_score": 0.658,
        "log_propability": -496.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.853,
        "cos_similarity": 0.8250830078125
    },
    "mmlu_pro_law_mini_gpt4": {
        "perplexity": 4.47683497428894,
        "IDF_score": 0.578,
        "log_propability": -346.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.94,
        "cos_similarity": 0.845234375
    },
    "mmlu_pro_law_groundtruth": {
        "perplexity": 18030.01772338867,
        "IDF_score": 28.0,
        "log_propability": -35.6,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.183,
        "cos_similarity": 0.160543212890625
    },
    "mmlu_pro_law_openai_human_written_examples": {
        "perplexity": 6.1716522073745725,
        "IDF_score": 0.585,
        "log_propability": -294.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.806,
        "cos_similarity": 0.828193359375
    }
}