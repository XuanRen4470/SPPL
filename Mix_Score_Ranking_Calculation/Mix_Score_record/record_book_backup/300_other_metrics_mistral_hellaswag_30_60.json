{
    "hellaswag_step_by_step": {
        "perplexity": 4.614414350191752,
        "IDF_score": 0.618,
        "log_propability": -472.0,
        "skywork_reward_score": 5.784023030598958,
        "CAR_score": 1.05
    },
    "hellaswag_claude": {
        "perplexity": 3.964733107884725,
        "IDF_score": 0.594,
        "log_propability": -252.0,
        "skywork_reward_score": 7.835416666666666,
        "CAR_score": 1.55
    },
    "hellaswag_gpt4_style_in_context_examples": {
        "perplexity": 4.092791835467021,
        "IDF_score": 0.601,
        "log_propability": -394.0,
        "skywork_reward_score": 6.583919270833333,
        "CAR_score": 1.28
    },
    "hellaswag_gpt4": {
        "perplexity": 7.811131270726522,
        "IDF_score": 0.702,
        "log_propability": -394.0,
        "skywork_reward_score": 4.183463541666667,
        "CAR_score": 0.61
    },
    "hellaswag_mini_gpt4": {
        "perplexity": 7.022703997294108,
        "IDF_score": 0.688,
        "log_propability": -273.0,
        "skywork_reward_score": 1.3033854166666667,
        "CAR_score": 0.195
    },
    "hellaswag_groundtruth": {
        "perplexity": 8448108.022058105,
        "IDF_score": 2.87,
        "log_propability": -28.3,
        "skywork_reward_score": -17.972916666666666,
        "CAR_score": -0.413
    },
    "hellaswag_openai_human_written_examples": {
        "perplexity": 6.815863005320231,
        "IDF_score": 0.619,
        "log_propability": -288.0,
        "skywork_reward_score": 6.113541666666666,
        "CAR_score": 0.924
    }
}