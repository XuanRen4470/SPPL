{
    "mmlu_moral_scenarios_step_by_step": {
        "perplexity": 3.815717325210571,
        "IDF_score": 0.543,
        "log_propability": -357.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.553,
        "cos_similarity": 0.8240738932291667
    },
    "mmlu_moral_scenarios_claude": {
        "perplexity": 2.960496120452881,
        "IDF_score": 0.399,
        "log_propability": -202.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.651,
        "cos_similarity": 0.8456901041666667
    },
    "mmlu_moral_scenarios_gpt4_style_in_context_examples": {
        "perplexity": 3.33587011496226,
        "IDF_score": 0.483,
        "log_propability": -317.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.599,
        "cos_similarity": 0.8186881510416667
    },
    "mmlu_moral_scenarios_gpt4": {
        "perplexity": 4.038759194215139,
        "IDF_score": 0.414,
        "log_propability": -255.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.536,
        "cos_similarity": 0.8487679036458333
    },
    "mmlu_moral_scenarios_mini_gpt4": {
        "perplexity": 3.770023416678111,
        "IDF_score": 0.369,
        "log_propability": -217.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.557,
        "cos_similarity": 0.8544970703125
    },
    "mmlu_moral_scenarios_groundtruth": {
        "perplexity": 51083.26365600586,
        "IDF_score": 88.9,
        "log_propability": -39.3,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.0899,
        "cos_similarity": 0.1980377197265625
    },
    "mmlu_moral_scenarios_openai_human_written_examples": {
        "perplexity": 4.8007857282956445,
        "IDF_score": 0.316,
        "log_propability": -169.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.494,
        "cos_similarity": 0.8247477213541666
    }
}