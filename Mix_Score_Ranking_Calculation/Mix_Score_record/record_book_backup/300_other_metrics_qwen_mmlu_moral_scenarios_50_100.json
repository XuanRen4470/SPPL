{
    "mmlu_moral_scenarios_step_by_step": {
        "perplexity": 3.0886928033828736,
        "IDF_score": 0.62,
        "log_propability": -240.0,
        "skywork_reward_score": 9.5821875,
        "CAR_score": 2.21
    },
    "mmlu_moral_scenarios_claude": {
        "perplexity": 2.6406749057769776,
        "IDF_score": 0.503,
        "log_propability": -154.0,
        "skywork_reward_score": 9.48234375,
        "CAR_score": 2.46
    },
    "mmlu_moral_scenarios_gpt4_style_in_context_examples": {
        "perplexity": 3.0261031007766723,
        "IDF_score": 0.593,
        "log_propability": -239.0,
        "skywork_reward_score": 10.1109765625,
        "CAR_score": 2.36
    },
    "mmlu_moral_scenarios_gpt4": {
        "perplexity": 3.4088160943984986,
        "IDF_score": 0.556,
        "log_propability": -181.0,
        "skywork_reward_score": 5.1252734375,
        "CAR_score": 1.11
    },
    "mmlu_moral_scenarios_mini_gpt4": {
        "perplexity": 2.860919089317322,
        "IDF_score": 0.497,
        "log_propability": -144.0,
        "skywork_reward_score": 5.276298828125,
        "CAR_score": 1.29
    },
    "mmlu_moral_scenarios_groundtruth": {
        "perplexity": 1757.5012762451172,
        "IDF_score": 2.05,
        "log_propability": -28.3,
        "skywork_reward_score": -6.0603125,
        "CAR_score": -0.272
    },
    "mmlu_moral_scenarios_openai_human_written_examples": {
        "perplexity": 4.020683221817016,
        "IDF_score": 0.505,
        "log_propability": -128.0,
        "skywork_reward_score": 2.79314453125,
        "CAR_score": 0.556
    }
}