{
    "mmlu_pro_law_step_by_step": {
        "perplexity": 4.691292204856873,
        "IDF_score": 0.738,
        "log_propability": -495.0,
        "skywork_reward_score": 7.5061328125,
        "CAR_score": 1.35
    },
    "mmlu_pro_law_claude": {
        "perplexity": 3.4293547534942626,
        "IDF_score": 0.678,
        "log_propability": -266.0,
        "skywork_reward_score": 3.71078125,
        "CAR_score": 0.8
    },
    "mmlu_pro_law_gpt4_style_in_context_examples": {
        "perplexity": 5.038822159767151,
        "IDF_score": 0.733,
        "log_propability": -414.0,
        "skywork_reward_score": 8.881640625,
        "CAR_score": 1.55
    },
    "mmlu_pro_law_gpt4": {
        "perplexity": 4.8466435098648075,
        "IDF_score": 0.705,
        "log_propability": -364.0,
        "skywork_reward_score": 5.16958984375,
        "CAR_score": 0.915
    },
    "mmlu_pro_law_mini_gpt4": {
        "perplexity": 4.549968504905701,
        "IDF_score": 0.683,
        "log_propability": -309.0,
        "skywork_reward_score": 3.814375,
        "CAR_score": 0.703
    },
    "mmlu_pro_law_groundtruth": {
        "perplexity": 40.34492824554443,
        "IDF_score": 0.615,
        "log_propability": -14.5,
        "skywork_reward_score": -16.000625,
        "CAR_score": -1.35
    },
    "mmlu_pro_law_openai_human_written_examples": {
        "perplexity": 5.512550225257874,
        "IDF_score": 0.668,
        "log_propability": -211.0,
        "skywork_reward_score": 3.321572265625,
        "CAR_score": 0.561
    }
}