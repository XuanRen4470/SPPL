{
    "mmlu_step_by_step": {
        "perplexity": 4.311354641914368,
        "IDF_score": 0.701,
        "log_propability": -427.0,
        "skywork_reward_score": 13.08826171875,
        "CAR_score": 2.46
    },
    "mmlu_claude": {
        "perplexity": 3.0970052170753477,
        "IDF_score": 0.639,
        "log_propability": -276.0,
        "skywork_reward_score": 15.346875,
        "CAR_score": 3.52
    },
    "mmlu_gpt4_style_in_context_examples": {
        "perplexity": 4.553973603248596,
        "IDF_score": 0.736,
        "log_propability": -552.0,
        "skywork_reward_score": 19.385625,
        "CAR_score": 3.54
    },
    "mmlu_gpt4": {
        "perplexity": 4.718385190963745,
        "IDF_score": 0.69,
        "log_propability": -364.0,
        "skywork_reward_score": 10.221953125,
        "CAR_score": 1.85
    },
    "mmlu_mini_gpt4": {
        "perplexity": 4.346792998313904,
        "IDF_score": 0.672,
        "log_propability": -315.0,
        "skywork_reward_score": 8.697255859375,
        "CAR_score": 1.62
    },
    "mmlu_groundtruth": {
        "perplexity": 30.35442419052124,
        "IDF_score": 0.615,
        "log_propability": -13.3,
        "skywork_reward_score": -5.7389453125,
        "CAR_score": -0.522
    },
    "mmlu_openai_human_written_examples": {
        "perplexity": 5.115820713043213,
        "IDF_score": 0.723,
        "log_propability": -458.0,
        "skywork_reward_score": 15.01453125,
        "CAR_score": 2.61
    }
}