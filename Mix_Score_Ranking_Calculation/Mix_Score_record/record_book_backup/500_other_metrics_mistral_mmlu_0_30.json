{
    "mmlu_step_by_step": {
        "perplexity": 4.300583728154501,
        "IDF_score": 0.66,
        "log_propability": -520.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.77,
        "cos_similarity": 0.8568522135416666
    },
    "mmlu_claude": {
        "perplexity": 3.0379697402318318,
        "IDF_score": 0.567,
        "log_propability": -311.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 3.42,
        "cos_similarity": 0.874365234375
    },
    "mmlu_gpt4_style_in_context_examples": {
        "perplexity": 4.046356360117595,
        "IDF_score": 0.645,
        "log_propability": -605.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.86,
        "cos_similarity": 0.852294921875
    },
    "mmlu_gpt4": {
        "perplexity": 4.844211570421854,
        "IDF_score": 0.607,
        "log_propability": -449.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.61,
        "cos_similarity": 0.86533203125
    },
    "mmlu_mini_gpt4": {
        "perplexity": 4.145880134900411,
        "IDF_score": 0.504,
        "log_propability": -316.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.82,
        "cos_similarity": 0.874072265625
    },
    "mmlu_groundtruth": {
        "perplexity": 55370.1946085612,
        "IDF_score": 104.0,
        "log_propability": -37.3,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 0.507,
        "cos_similarity": 0.13616536458333334
    },
    "mmlu_openai_human_written_examples": {
        "perplexity": 5.088367605209351,
        "IDF_score": 0.65,
        "log_propability": -542.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.53,
        "cos_similarity": 0.8563802083333333
    }
}