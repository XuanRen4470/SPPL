{
    "hellaswag_step_by_step": {
        "perplexity": 4.919657001495361,
        "IDF_score": 0.461,
        "log_propability": -468.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 0.997,
        "cos_similarity": 0.761201171875
    },
    "hellaswag_claude": {
        "perplexity": 4.009837837219238,
        "IDF_score": 0.421,
        "log_propability": -250.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 1.12,
        "cos_similarity": 0.76947265625
    },
    "hellaswag_gpt4_style_in_context_examples": {
        "perplexity": 3.9491897344589235,
        "IDF_score": 0.414,
        "log_propability": -381.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 1.12,
        "cos_similarity": 0.75724609375
    },
    "hellaswag_gpt4": {
        "perplexity": 6.754852046966553,
        "IDF_score": 0.447,
        "log_propability": -356.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 0.869,
        "cos_similarity": 0.761884765625
    },
    "hellaswag_mini_gpt4": {
        "perplexity": 7.066375880241394,
        "IDF_score": 0.449,
        "log_propability": -267.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 0.853,
        "cos_similarity": 0.774443359375
    },
    "hellaswag_groundtruth": {
        "perplexity": 32032414.416291505,
        "IDF_score": 176000.0,
        "log_propability": -27.3,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 0.136,
        "cos_similarity": 0.1293218994140625
    },
    "hellaswag_openai_human_written_examples": {
        "perplexity": 6.544364824295044,
        "IDF_score": 0.353,
        "log_propability": -274.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 0.878,
        "cos_similarity": 0.780498046875
    }
}