{
    "mmlu_step_by_step": {
        "perplexity": 4.594351037740707,
        "IDF_score": 0.676,
        "log_propability": -521.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.67,
        "cos_similarity": 0.8494921875
    },
    "mmlu_claude": {
        "perplexity": 3.1785795867443083,
        "IDF_score": 0.584,
        "log_propability": -311.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 3.32,
        "cos_similarity": 0.8687841796875
    },
    "mmlu_gpt4_style_in_context_examples": {
        "perplexity": 4.16076031923294,
        "IDF_score": 0.692,
        "log_propability": -608.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.83,
        "cos_similarity": 0.8410107421875
    },
    "mmlu_gpt4": {
        "perplexity": 4.846905484199524,
        "IDF_score": 0.622,
        "log_propability": -404.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.61,
        "cos_similarity": 0.8659423828125
    },
    "mmlu_mini_gpt4": {
        "perplexity": 4.213280987739563,
        "IDF_score": 0.531,
        "log_propability": -320.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.81,
        "cos_similarity": 0.8696533203125
    },
    "mmlu_groundtruth": {
        "perplexity": 32107.527552490235,
        "IDF_score": 59.9,
        "log_propability": -35.3,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 0.534,
        "cos_similarity": 0.1409791564941406
    },
    "mmlu_openai_human_written_examples": {
        "perplexity": 4.730111103057862,
        "IDF_score": 0.651,
        "log_propability": -502.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.65,
        "cos_similarity": 0.853623046875
    }
}