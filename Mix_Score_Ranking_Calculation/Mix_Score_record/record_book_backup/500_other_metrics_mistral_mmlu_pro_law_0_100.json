{
    "mmlu_pro_law_step_by_step": {
        "perplexity": 4.731505708694458,
        "IDF_score": 0.703,
        "log_propability": -603.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.911,
        "cos_similarity": 0.8077734375
    },
    "mmlu_pro_law_claude": {
        "perplexity": 3.359778592586517,
        "IDF_score": 0.628,
        "log_propability": -324.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 1.11,
        "cos_similarity": 0.8349267578125
    },
    "mmlu_pro_law_gpt4_style_in_context_examples": {
        "perplexity": 5.151185591220855,
        "IDF_score": 0.65,
        "log_propability": -481.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.879,
        "cos_similarity": 0.822470703125
    },
    "mmlu_pro_law_gpt4": {
        "perplexity": 5.473709557056427,
        "IDF_score": 0.661,
        "log_propability": -465.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.849,
        "cos_similarity": 0.82323486328125
    },
    "mmlu_pro_law_mini_gpt4": {
        "perplexity": 4.603519175052643,
        "IDF_score": 0.576,
        "log_propability": -349.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.928,
        "cos_similarity": 0.83962158203125
    },
    "mmlu_pro_law_groundtruth": {
        "perplexity": 27089.563756713866,
        "IDF_score": 48.4,
        "log_propability": -36.6,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.178,
        "cos_similarity": 0.16064727783203125
    },
    "mmlu_pro_law_openai_human_written_examples": {
        "perplexity": 6.665271315574646,
        "IDF_score": 0.598,
        "log_propability": -281.0,
        "skywork_reward_score": 5.06822998046875,
        "CAR_score": 0.784,
        "cos_similarity": 0.8235302734375
    }
}