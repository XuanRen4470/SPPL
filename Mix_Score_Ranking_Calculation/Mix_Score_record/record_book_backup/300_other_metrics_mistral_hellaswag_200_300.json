{
    "hellaswag_step_by_step": {
        "perplexity": 4.924628562927246,
        "IDF_score": 0.638,
        "log_propability": -441.0,
        "skywork_reward_score": 5.3801953125,
        "CAR_score": 0.945
    },
    "hellaswag_claude": {
        "perplexity": 4.058293995857238,
        "IDF_score": 0.587,
        "log_propability": -240.0,
        "skywork_reward_score": 6.9931787109375,
        "CAR_score": 1.36
    },
    "hellaswag_gpt4_style_in_context_examples": {
        "perplexity": 3.808002517223358,
        "IDF_score": 0.586,
        "log_propability": -382.0,
        "skywork_reward_score": 5.72791015625,
        "CAR_score": 1.15
    },
    "hellaswag_gpt4": {
        "perplexity": 6.74100903749466,
        "IDF_score": 0.684,
        "log_propability": -387.0,
        "skywork_reward_score": 5.6019970703125,
        "CAR_score": 0.861
    },
    "hellaswag_mini_gpt4": {
        "perplexity": 6.112667798995972,
        "IDF_score": 0.659,
        "log_propability": -294.0,
        "skywork_reward_score": 3.7022412109375,
        "CAR_score": 0.588
    },
    "hellaswag_groundtruth": {
        "perplexity": 14046628.259215089,
        "IDF_score": 2.75,
        "log_propability": -26.4,
        "skywork_reward_score": -17.644375,
        "CAR_score": -0.435
    },
    "hellaswag_openai_human_written_examples": {
        "perplexity": 6.447723019123077,
        "IDF_score": 0.623,
        "log_propability": -283.0,
        "skywork_reward_score": 5.715357055664063,
        "CAR_score": 0.889
    }
}