{
    "mmlu_moral_scenarios_step_by_step": {
        "perplexity": 3.8684356844425203,
        "IDF_score": 0.545,
        "log_propability": -358.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.549,
        "cos_similarity": 0.8228759765625
    },
    "mmlu_moral_scenarios_claude": {
        "perplexity": 2.981273807287216,
        "IDF_score": 0.399,
        "log_propability": -202.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.648,
        "cos_similarity": 0.84229248046875
    },
    "mmlu_moral_scenarios_gpt4_style_in_context_examples": {
        "perplexity": 3.3484629249572753,
        "IDF_score": 0.487,
        "log_propability": -318.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.598,
        "cos_similarity": 0.81531005859375
    },
    "mmlu_moral_scenarios_gpt4": {
        "perplexity": 4.052449103593826,
        "IDF_score": 0.413,
        "log_propability": -255.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.535,
        "cos_similarity": 0.84614501953125
    },
    "mmlu_moral_scenarios_mini_gpt4": {
        "perplexity": 3.7033222913742065,
        "IDF_score": 0.364,
        "log_propability": -212.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.563,
        "cos_similarity": 0.85233642578125
    },
    "mmlu_moral_scenarios_groundtruth": {
        "perplexity": 55388.8523614502,
        "IDF_score": 95.0,
        "log_propability": -39.8,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.089,
        "cos_similarity": 0.198992919921875
    },
    "mmlu_moral_scenarios_openai_human_written_examples": {
        "perplexity": 4.767742816209793,
        "IDF_score": 0.322,
        "log_propability": -173.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.495,
        "cos_similarity": 0.82344970703125
    }
}