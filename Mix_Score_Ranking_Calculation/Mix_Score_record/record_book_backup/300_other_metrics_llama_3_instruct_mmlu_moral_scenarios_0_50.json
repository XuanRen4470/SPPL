{
    "mmlu_moral_scenarios_step_by_step": {
        "perplexity": 3.4992103290557863,
        "IDF_score": 0.578,
        "log_propability": -285.0,
        "skywork_reward_score": 8.683876953125,
        "CAR_score": 1.85
    },
    "mmlu_moral_scenarios_claude": {
        "perplexity": 3.0130170822143554,
        "IDF_score": 0.497,
        "log_propability": -173.0,
        "skywork_reward_score": 9.4371875,
        "CAR_score": 2.21
    },
    "mmlu_moral_scenarios_gpt4_style_in_context_examples": {
        "perplexity": 3.7459523344039916,
        "IDF_score": 0.597,
        "log_propability": -291.0,
        "skywork_reward_score": 8.11249267578125,
        "CAR_score": 1.65
    },
    "mmlu_moral_scenarios_gpt4": {
        "perplexity": 3.57286452293396,
        "IDF_score": 0.515,
        "log_propability": -199.0,
        "skywork_reward_score": 3.61876953125,
        "CAR_score": 0.76
    },
    "mmlu_moral_scenarios_mini_gpt4": {
        "perplexity": 3.7536336994171142,
        "IDF_score": 0.538,
        "log_propability": -187.0,
        "skywork_reward_score": 5.741201171875,
        "CAR_score": 1.17
    },
    "mmlu_moral_scenarios_groundtruth": {
        "perplexity": 24.973536605834962,
        "IDF_score": 0.591,
        "log_propability": -12.8,
        "skywork_reward_score": -8.341923828125,
        "CAR_score": -0.787
    },
    "mmlu_moral_scenarios_openai_human_written_examples": {
        "perplexity": 4.059707403182983,
        "IDF_score": 0.477,
        "log_propability": -134.0,
        "skywork_reward_score": 2.090390625,
        "CAR_score": 0.409
    }
}