{
    "hellaswag_step_by_step": {
        "perplexity": 4.771624233722687,
        "IDF_score": 0.447,
        "log_propability": -448.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 1.02,
        "cos_similarity": 0.767509765625
    },
    "hellaswag_claude": {
        "perplexity": 3.968514814376831,
        "IDF_score": 0.408,
        "log_propability": -245.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 1.12,
        "cos_similarity": 0.77305908203125
    },
    "hellaswag_gpt4_style_in_context_examples": {
        "perplexity": 3.957936086654663,
        "IDF_score": 0.425,
        "log_propability": -386.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 1.12,
        "cos_similarity": 0.77095703125
    },
    "hellaswag_gpt4": {
        "perplexity": 6.948824815750122,
        "IDF_score": 0.465,
        "log_propability": -352.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 0.862,
        "cos_similarity": 0.7722705078125
    },
    "hellaswag_mini_gpt4": {
        "perplexity": 6.325911848545075,
        "IDF_score": 0.421,
        "log_propability": -275.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 0.888,
        "cos_similarity": 0.77578369140625
    },
    "hellaswag_groundtruth": {
        "perplexity": 6028078.842945557,
        "IDF_score": 37800.0,
        "log_propability": -26.3,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 0.141,
        "cos_similarity": 0.1245672607421875
    },
    "hellaswag_openai_human_written_examples": {
        "perplexity": 5.694954991340637,
        "IDF_score": 0.333,
        "log_propability": -260.0,
        "skywork_reward_score": 5.702202351888021,
        "CAR_score": 0.937,
        "cos_similarity": 0.7850732421875
    }
}