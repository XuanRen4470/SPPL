{
    "mmlu_moral_scenarios_step_by_step": {
        "perplexity": 3.4522961521148683,
        "IDF_score": 0.571,
        "log_propability": -264.0,
        "skywork_reward_score": 9.5821875,
        "CAR_score": 2.06
    },
    "mmlu_moral_scenarios_claude": {
        "perplexity": 3.130251450538635,
        "IDF_score": 0.5,
        "log_propability": -180.0,
        "skywork_reward_score": 9.48234375,
        "CAR_score": 2.17
    },
    "mmlu_moral_scenarios_gpt4_style_in_context_examples": {
        "perplexity": 3.634869589805603,
        "IDF_score": 0.593,
        "log_propability": -277.0,
        "skywork_reward_score": 10.1109765625,
        "CAR_score": 2.09
    },
    "mmlu_moral_scenarios_gpt4": {
        "perplexity": 3.5743455934524535,
        "IDF_score": 0.506,
        "log_propability": -188.0,
        "skywork_reward_score": 5.1252734375,
        "CAR_score": 1.07
    },
    "mmlu_moral_scenarios_mini_gpt4": {
        "perplexity": 3.4448421239852904,
        "IDF_score": 0.511,
        "log_propability": -170.0,
        "skywork_reward_score": 5.276298828125,
        "CAR_score": 1.13
    },
    "mmlu_moral_scenarios_groundtruth": {
        "perplexity": 26.058646354675293,
        "IDF_score": 0.598,
        "log_propability": -13.0,
        "skywork_reward_score": -6.0603125,
        "CAR_score": -0.564
    },
    "mmlu_moral_scenarios_openai_human_written_examples": {
        "perplexity": 3.792603225708008,
        "IDF_score": 0.451,
        "log_propability": -124.0,
        "skywork_reward_score": 2.79314453125,
        "CAR_score": 0.569
    }
}