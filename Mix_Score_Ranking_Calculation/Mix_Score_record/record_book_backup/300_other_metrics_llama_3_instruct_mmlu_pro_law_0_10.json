{
    "mmlu_pro_law_step_by_step": {
        "perplexity": 5.01879472732544,
        "IDF_score": 0.766,
        "log_propability": -619.0,
        "skywork_reward_score": 9.73359375,
        "CAR_score": 1.69
    },
    "mmlu_pro_law_claude": {
        "perplexity": 3.1660810708999634,
        "IDF_score": 0.664,
        "log_propability": -255.0,
        "skywork_reward_score": 8.191015625,
        "CAR_score": 1.84
    },
    "mmlu_pro_law_gpt4_style_in_context_examples": {
        "perplexity": 4.543476557731628,
        "IDF_score": 0.721,
        "log_propability": -363.0,
        "skywork_reward_score": 10.41484375,
        "CAR_score": 1.9
    },
    "mmlu_pro_law_gpt4": {
        "perplexity": 5.879832935333252,
        "IDF_score": 0.776,
        "log_propability": -553.0,
        "skywork_reward_score": 4.47109375,
        "CAR_score": 0.718
    },
    "mmlu_pro_law_mini_gpt4": {
        "perplexity": 4.422020626068115,
        "IDF_score": 0.668,
        "log_propability": -296.0,
        "skywork_reward_score": 2.8080078125,
        "CAR_score": 0.522
    },
    "mmlu_pro_law_groundtruth": {
        "perplexity": 42.01039810180664,
        "IDF_score": 0.638,
        "log_propability": -14.8,
        "skywork_reward_score": -17.125,
        "CAR_score": -1.42
    },
    "mmlu_pro_law_openai_human_written_examples": {
        "perplexity": 5.233975744247436,
        "IDF_score": 0.651,
        "log_propability": -227.0,
        "skywork_reward_score": 4.821240234375,
        "CAR_score": 0.831
    }
}