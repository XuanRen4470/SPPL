{
    "mmlu_step_by_step": {
        "perplexity": 3.9490518013636273,
        "IDF_score": 0.736,
        "log_propability": -408.0,
        "skywork_reward_score": 12.453645833333333,
        "CAR_score": 2.47
    },
    "mmlu_claude": {
        "perplexity": 2.793168826897939,
        "IDF_score": 0.638,
        "log_propability": -250.0,
        "skywork_reward_score": 15.31875,
        "CAR_score": 3.8
    },
    "mmlu_gpt4_style_in_context_examples": {
        "perplexity": 3.8193305412928265,
        "IDF_score": 0.727,
        "log_propability": -485.0,
        "skywork_reward_score": 17.430598958333334,
        "CAR_score": 3.63
    },
    "mmlu_gpt4": {
        "perplexity": 4.25223122437795,
        "IDF_score": 0.699,
        "log_propability": -346.0,
        "skywork_reward_score": 9.480208333333334,
        "CAR_score": 1.82
    },
    "mmlu_mini_gpt4": {
        "perplexity": 3.471497106552124,
        "IDF_score": 0.619,
        "log_propability": -254.0,
        "skywork_reward_score": 7.302197265625,
        "CAR_score": 1.56
    },
    "mmlu_groundtruth": {
        "perplexity": 4510.828140258789,
        "IDF_score": 2.18,
        "log_propability": -30.1,
        "skywork_reward_score": -6.127864583333333,
        "CAR_score": -0.26
    },
    "mmlu_openai_human_written_examples": {
        "perplexity": 3.994987932840983,
        "IDF_score": 0.694,
        "log_propability": -376.0,
        "skywork_reward_score": 13.155305989583333,
        "CAR_score": 2.62
    }
}