{
    "mmlu_moral_scenarios_step_by_step": {
        "perplexity": 3.1994330056508384,
        "IDF_score": 0.636,
        "log_propability": -268.0,
        "skywork_reward_score": 9.260763346354167,
        "CAR_score": 2.09
    },
    "mmlu_moral_scenarios_claude": {
        "perplexity": 2.550289374987284,
        "IDF_score": 0.492,
        "log_propability": -148.0,
        "skywork_reward_score": 8.704588216145833,
        "CAR_score": 2.31
    },
    "mmlu_moral_scenarios_gpt4_style_in_context_examples": {
        "perplexity": 3.125995758374532,
        "IDF_score": 0.6,
        "log_propability": -249.0,
        "skywork_reward_score": 8.975193277994792,
        "CAR_score": 2.05
    },
    "mmlu_moral_scenarios_gpt4": {
        "perplexity": 3.3814873623847963,
        "IDF_score": 0.568,
        "log_propability": -192.0,
        "skywork_reward_score": 4.5792626953125,
        "CAR_score": 0.996
    },
    "mmlu_moral_scenarios_mini_gpt4": {
        "perplexity": 3.0080085444450377,
        "IDF_score": 0.51,
        "log_propability": -155.0,
        "skywork_reward_score": 4.922395426432292,
        "CAR_score": 1.16
    },
    "mmlu_moral_scenarios_groundtruth": {
        "perplexity": 1706.9223533121744,
        "IDF_score": 2.02,
        "log_propability": -28.0,
        "skywork_reward_score": -7.52003173828125,
        "CAR_score": -0.341
    },
    "mmlu_moral_scenarios_openai_human_written_examples": {
        "perplexity": 3.9928329849243163,
        "IDF_score": 0.505,
        "log_propability": -128.0,
        "skywork_reward_score": 2.7423811848958333,
        "CAR_score": 0.543
    }
}