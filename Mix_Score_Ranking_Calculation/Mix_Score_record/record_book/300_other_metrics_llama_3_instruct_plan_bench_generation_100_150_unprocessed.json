{
    "plan_bench_generation_step_by_step": {
        "perplexity": 2.2300797796249388,
        "IDF_score": 0.639,
        "log_propability": -284.0,
        "skywork_reward_score": -0.7708984375,
        "CAR_score": -0.233
    },
    "plan_bench_generation_claude": {
        "perplexity": 2.173545536994934,
        "IDF_score": 0.6,
        "log_propability": -165.0,
        "skywork_reward_score": -3.1074609375,
        "CAR_score": -0.941
    },
    "plan_bench_generation_gpt4_style_in_context_examples": {
        "perplexity": 2.132086696624756,
        "IDF_score": 0.648,
        "log_propability": -309.0,
        "skywork_reward_score": 1.27417724609375,
        "CAR_score": 0.396
    },
    "plan_bench_generation_gpt4": {
        "perplexity": 2.364937033653259,
        "IDF_score": 0.64,
        "log_propability": -261.0,
        "skywork_reward_score": -0.7594873046875,
        "CAR_score": -0.215
    },
    "plan_bench_generation_mini_gpt4": {
        "perplexity": 2.765764281749725,
        "IDF_score": 0.686,
        "log_propability": -320.0,
        "skywork_reward_score": -4.308291015625,
        "CAR_score": -1.09
    },
    "plan_bench_generation_groundtruth": {
        "perplexity": 14.147108478546143,
        "IDF_score": 0.616,
        "log_propability": -70.7,
        "skywork_reward_score": -12.07,
        "CAR_score": -1.72
    },
    "plan_bench_generation_openai_human_written_examples": {
        "perplexity": 2.616906440258026,
        "IDF_score": 0.656,
        "log_propability": -272.0,
        "skywork_reward_score": -1.16218505859375,
        "CAR_score": -0.307
    },
    "plan_bench_generation_rewrite_groundtruth_in_own_words": {
        "perplexity": 2.88585266828537,
        "IDF_score": 0.688,
        "log_propability": -205.0,
        "skywork_reward_score": -8.278359375,
        "CAR_score": -2.05
    }
}