{
    "mmlu_moral_scenarios_step_by_step": {
        "perplexity": 3.298742618560791,
        "IDF_score": 0.64,
        "log_propability": -268.0,
        "skywork_reward_score": 9.1330322265625,
        "CAR_score": 2.02
    },
    "mmlu_moral_scenarios_claude": {
        "perplexity": 2.5968217039108277,
        "IDF_score": 0.499,
        "log_propability": -151.0,
        "skywork_reward_score": 9.459765625,
        "CAR_score": 2.48
    },
    "mmlu_moral_scenarios_gpt4_style_in_context_examples": {
        "perplexity": 3.0952047085762024,
        "IDF_score": 0.6,
        "log_propability": -248.0,
        "skywork_reward_score": 9.111734619140625,
        "CAR_score": 2.09
    },
    "mmlu_moral_scenarios_gpt4": {
        "perplexity": 3.3584425115585326,
        "IDF_score": 0.561,
        "log_propability": -184.0,
        "skywork_reward_score": 4.372021484375,
        "CAR_score": 0.957
    },
    "mmlu_moral_scenarios_mini_gpt4": {
        "perplexity": 2.9546815872192385,
        "IDF_score": 0.508,
        "log_propability": -151.0,
        "skywork_reward_score": 5.50875,
        "CAR_score": 1.31
    },
    "mmlu_moral_scenarios_groundtruth": {
        "perplexity": 1770.0767248535155,
        "IDF_score": 2.04,
        "log_propability": -28.2,
        "skywork_reward_score": -7.2011181640625,
        "CAR_score": -0.325
    },
    "mmlu_moral_scenarios_openai_human_written_examples": {
        "perplexity": 4.036964755058289,
        "IDF_score": 0.511,
        "log_propability": -131.0,
        "skywork_reward_score": 2.441767578125,
        "CAR_score": 0.483
    }
}