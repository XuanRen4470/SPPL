{
    "mmlu_step_by_step": {
        "perplexity": 4.51381383061409,
        "IDF_score": 0.782,
        "log_propability": -525.0,
        "skywork_reward_score": 12.663734537760417,
        "CAR_score": 2.32
    },
    "mmlu_claude": {
        "perplexity": 3.1357607209682463,
        "IDF_score": 0.672,
        "log_propability": -315.0,
        "skywork_reward_score": 15.092535807291666,
        "CAR_score": 3.44
    },
    "mmlu_gpt4_style_in_context_examples": {
        "perplexity": 4.240280958811442,
        "IDF_score": 0.781,
        "log_propability": -607.0,
        "skywork_reward_score": 19.2316015625,
        "CAR_score": 3.67
    },
    "mmlu_gpt4": {
        "perplexity": 4.919240841070811,
        "IDF_score": 0.76,
        "log_propability": -427.0,
        "skywork_reward_score": 10.467571614583333,
        "CAR_score": 1.85
    },
    "mmlu_mini_gpt4": {
        "perplexity": 4.318308103879293,
        "IDF_score": 0.693,
        "log_propability": -331.0,
        "skywork_reward_score": 8.570555419921876,
        "CAR_score": 1.61
    },
    "mmlu_groundtruth": {
        "perplexity": 38836.70559326172,
        "IDF_score": 1.46,
        "log_propability": -36.5,
        "skywork_reward_score": -5.17563741048177,
        "CAR_score": -0.183
    },
    "mmlu_openai_human_written_examples": {
        "perplexity": 4.824680323600769,
        "IDF_score": 0.772,
        "log_propability": -515.0,
        "skywork_reward_score": 14.687073364257813,
        "CAR_score": 2.62
    }
}